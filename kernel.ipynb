{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport ast\nfrom glob import glob\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\ntrain_files = glob(\"../input/train_simplified/*.csv\")\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['test_simplified.csv', 'sample_submission.csv', 'train_simplified', 'test_raw.csv']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a9554ef02ec541b42d01308cf20d139d80e84199"
      },
      "cell_type": "code",
      "source": "# Reading training data\n# NROWS=1200\n# data = pd.DataFrame()\n# for f in train_files:\n#     data = data.append(pd.read_csv(f, \n#                    index_col=\"key_id\", nrows=NROWS))\n\n# data[\"word\"] = data[\"word\"].replace(\"\\s+\", \"_\", regex=True)\n\n# data = data.sample(frac=1, replace=False)\n# data.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c63a9e04b3386e2d88b1d01000dff353d58a5ea9"
      },
      "cell_type": "code",
      "source": "# Reading test file\ntest = pd.read_csv('../input/test_simplified.csv', index_col=\"key_id\")\ntest.head()\ntestidx = test.index",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a15225ed223f907911185dee8ce7d01f0240abcd"
      },
      "cell_type": "code",
      "source": "print_ids = data.iloc[:100].index\ndata_images = [ast.literal_eval(lst) for lst in data.loc[print_ids, 'drawing'].values]\n# Plot parrot figures, coloring as green if they're recognized,\n# red otherwise.\nplt.figure(figsize=(16, 16))\nfor index, image in enumerate(data_images):\n    for x, y in image:\n        plt.subplot(10, 10, index+1)\n        col = \"green\" if data[\"recognized\"].iloc[index] else \"red\"\n        plt.gca().invert_yaxis()\n        plt.axis(\"off\")\n        plt.plot(x, y, marker=\".\", color=col)\n\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "af09e4f105cfb6f5d01675128d1edc192f34dfe5"
      },
      "cell_type": "code",
      "source": "# Rasterizing vector images\nfrom PIL import Image, ImageDraw\nfrom dask import bag\n\n# Reading training data\n# NROWS=1200\n# data = pd.DataFrame()\n# for f in train_files:\n#     data = data.append(pd.read_csv(f, \n#                    index_col=\"key_id\", nrows=NROWS))\n\n# data[\"word\"] = data[\"word\"].replace(\"\\s+\", \"_\", regex=True)\n\n# data = data.sample(frac=1, replace=False)\n# data.head()\n\nSIZE = 32\n# Rasterizarion algorithm from https://www.kaggle.com/jpmiller/image-based-cnn#\ndef draw_it(strokes, imheight=32, imwidth=32):\n    image = Image.new(\"P\", (256,256), color=255)\n    image_draw = ImageDraw.Draw(image)\n    for stroke in ast.literal_eval(strokes):\n        for i in range(len(stroke[0])-1):\n            image_draw.line([stroke[0][i], \n                             stroke[1][i],\n                             stroke[0][i+1], \n                             stroke[1][i+1]],\n                            fill=0, width=5)\n    image = image.resize((imheight, imwidth))\n    return np.array(image)/255.\n\n# imagebag = bag.from_sequence(data.drawing.values).map(draw_it, \n#                                                       imheight=SIZE, \n#                                                       imwidth=SIZE)\n# raster_array = np.array(imagebag.compute())\n\nimagebag = bag.from_sequence(test.drawing.values).map(draw_it, \n                                                      imheight=SIZE, \n                                                      imwidth=SIZE)\n\ntest_X = np.array(imagebag.compute())",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6ba506df4484a08f1840b7a4a12a0bba8f000b98"
      },
      "cell_type": "code",
      "source": "NUM_ROWS_TOTAL = 49707919//60\nNUM_ROWS_TOTAL -= (NUM_ROWS_TOTAL % 340)\nSIZE = 32\nraster_array = np.zeros((NUM_ROWS_TOTAL, SIZE, SIZE, 1))\nraster_array.shape",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/plain": "(828240, 32, 32, 1)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "19190665b05a5ca3e9bc9f367d29bb289d52fa3d"
      },
      "cell_type": "code",
      "source": "CHUNKSIZE=1024\nSIZE=32\nEACH_SET_SIZE = NUM_ROWS_TOTAL//340\n\ni = 0\nfor f in train_files:\n    for df in pd.read_csv(f, index_col=\"key_id\", chunksize=CHUNKSIZE, nrows=EACH_SET_SIZE):\n        imagebag = bag.from_sequence(df.drawing.values).map(draw_it, imheight=SIZE, imwidth=SIZE)\n        imagebag = np.array(imagebag.compute())\n        raster_array[i:(i + imagebag.shape[0])] = imagebag.reshape((*imagebag.shape, 1))\n        i += imagebag.shape[0]\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "86f2e078a5ca9e6d09ad1639b15f20697503f655"
      },
      "cell_type": "code",
      "source": "# Print rasterized images\nplt.figure(figsize=(16, 16))\nfor index, image in enumerate(raster_array[:100]):\n    plt.subplot(10, 10, index+1)\n    col = \"winter\" if data[\"recognized\"].iloc[index] else \"Oranges\"\n    plt.gca().invert_yaxis()\n    plt.axis(\"off\")\n    plt.imshow(image, cmap=col)\n\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6119819ce282009a701b9e006c9a4ca80ba395e6"
      },
      "cell_type": "code",
      "source": "raster_array = raster_array.reshape(-1, SIZE,SIZE, 1)\ntest_X = test_X.reshape(-1, SIZE,SIZE, 1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4ee9c9edc5ced73c6e5b5c003db09cd606064438"
      },
      "cell_type": "code",
      "source": "# get dummies from classes and delete dataframes for memory\ndummies = pd.get_dummies(data['word'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1be6849ce1d361f42640c077ab474da83920723e"
      },
      "cell_type": "code",
      "source": "del data, test",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "202dee57861dc49e43105aa3000212d518d3d85b"
      },
      "cell_type": "code",
      "source": "# Split training data for validation\nfrom sklearn.model_selection import train_test_split\ntrain_X,valid_X,train_label,valid_label = train_test_split(raster_array, dummies, test_size=0.2, random_state=13)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0b74bf366479ba17ba8e6d12c4f3dc7d047c7516"
      },
      "cell_type": "code",
      "source": "train_X.shape,valid_X.shape,train_label.shape,valid_label.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d84218dbd83f23af14828fe35f5c2d2b9c8d8259"
      },
      "cell_type": "code",
      "source": "# Constants\nBATCH_SIZE = 64\nEPOCHS = 40\nNUM_CLASSES = 340\nSHAPE = (SIZE, SIZE, 1)\nFILTER_SIZE = (4, 4)\nPROB_DO_HIDDEN = 0.3",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4338e7578cf0fbeda4016b8a758ba91ba6802116"
      },
      "cell_type": "code",
      "source": "import keras\nfrom keras.models import Sequential,Input,Model\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.layers import Activation\n\n\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size=FILTER_SIZE, strides=1, input_shape=SHAPE))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(PROB_DO_HIDDEN))\n\nmodel.add(Conv2D(64, kernel_size=FILTER_SIZE, strides=1))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(PROB_DO_HIDDEN))\n\n# model.add(Conv2D(128, kernel_size=FILTER_SIZE, strides=1))\n# model.add(Activation('relu'))\n# model.add(MaxPooling2D(pool_size=(2, 2)))\n# model.add(Dropout(PROB_DO_HIDDEN))\n\nmodel.add(Flatten())\nmodel.add(Dense(512))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(PROB_DO_HIDDEN))\n\nmodel.add(Dense(NUM_CLASSES))\nmodel.add(Activation('softmax'))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9a713d4727742c3ec89cedba13b137111d5cb65c"
      },
      "cell_type": "code",
      "source": "# Fonte: https://stackoverflow.com/questions/42327006/how-to-calculate-top5-accuracy-in-keras\nimport functools\ntop3_acc = functools.partial(keras.metrics.top_k_categorical_accuracy, k=3)\n\ntop3_acc.__name__ = 'top3_acc'",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a3b7b43d0f23735a46c1f91828d7d4d2a663fa21"
      },
      "cell_type": "code",
      "source": "model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(), metrics=['accuracy', top3_acc])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "33d0aec247a4f4833d0101e44052cf997b3ae183"
      },
      "cell_type": "code",
      "source": "model.summary()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4fb74b7c4fe51d44a36cd80fa3514f4cd30ba3ab"
      },
      "cell_type": "code",
      "source": "from keras.callbacks import EarlyStopping\nearly_stopping_monitor = EarlyStopping(patience=int(EPOCHS*0.2))\n\nhistory = model.fit(train_X, train_label, batch_size=BATCH_SIZE,epochs=EPOCHS,verbose=1,validation_data=(valid_X, valid_label), callbacks=[early_stopping_monitor])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "240cda22226670911049ed4fea322fff306e7f01"
      },
      "cell_type": "code",
      "source": "# Plot training & validation accuracy values\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot training & validation accuracy values\nplt.plot(history.history['top3_acc'])\nplt.plot(history.history['val_top3_acc'])\nplt.title('Model accuracy (top 3)')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7a6f1b6e632e57ebaf80c9b177cccc136bdded07"
      },
      "cell_type": "code",
      "source": "# Predict test dataset with model\ntest_Y = model.predict(test_X, batch_size=BATCH_SIZE)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ebe64cc4b4e1b0d376b8bccc898fbbd2677ac048"
      },
      "cell_type": "code",
      "source": "# Get top 3 classes\nans = np.argsort(-test_Y)[:,:3]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "96849028a84b369ece6146b525b4cc5b744f3a27"
      },
      "cell_type": "code",
      "source": "ans",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2c19c22c555a9e895228ba749b470080e70ea09e"
      },
      "cell_type": "code",
      "source": "# Generating output\nout_aux = []\nfor id, i in zip(testidx, dummies.columns[ans]):\n    out_aux.append([id, \" \".join(map(str, i))])\n\noutput = pd.DataFrame(columns=['key_id', 'word'], data=out_aux)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c74b12ecd24d69d3e3d955b89204121af5fce15c"
      },
      "cell_type": "code",
      "source": "output.to_csv(\"submission.csv\")",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}